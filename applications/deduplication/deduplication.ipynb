{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Conv1D, Dropout, Embedding, Input, Concatenate\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../../datasets/quora/train_quora.csv')\n",
    "test_df = pd.read_csv('../../datasets/quora/test_quora.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([train_df, test_df])[['question1', 'question2']].drop_duplicates().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.fillna('')\n",
    "test_df = test_df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_df[['question1', 'question2']]\n",
    "y_train = train_df['is_duplicate']\n",
    "\n",
    "X_test = test_df[['question1', 'question2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=1000)\n",
    "\n",
    "all_texts = pd.concat(\n",
    "    [\n",
    "        X_train['question1'], \n",
    "        X_train['question2'],\n",
    "        X_test['question1'], \n",
    "        X_test['question2']\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(all_texts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(all_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized = pd.DataFrame({\n",
    "    'question1': tokenizer.texts_to_sequences(X_train['question1']),\n",
    "    'question2': tokenizer.texts_to_sequences(X_train['question2'])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tokenized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1_array_train = sequence.pad_sequences(X_train_tokenized['question1'], maxlen=200)\n",
    "question2_array_train = sequence.pad_sequences(X_train_tokenized['question2'], maxlen=200)\n",
    "\n",
    "question1_array_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question1_array_train.shape)\n",
    "print(question2_array_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tokenized = pd.DataFrame({\n",
    "    'question1': tokenizer.texts_to_sequences(X_test['question1']),\n",
    "    'question2': tokenizer.texts_to_sequences(X_test['question2'])\n",
    "})\n",
    "\n",
    "question1_array_test = sequence.pad_sequences(X_test_tokenized['question1'], maxlen=200)\n",
    "question2_array_test = sequence.pad_sequences(X_test_tokenized['question2'], maxlen=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question1_array_test.shape)\n",
    "print(question2_array_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1_inp = Input(shape=(200,), name='q1_token_sequence')\n",
    "emb_question1 = Embedding(1000, 64, input_length=200, name='q1_embedding')(question1_inp)\n",
    "lstm_out_qst1 = LSTM(128, name='q1_lstm')(emb_question1)\n",
    "dropout_qst1 = Dropout(0.25, name='q1_dropout')(lstm_out_qst1)\n",
    "\n",
    "question2_inp = Input(shape=(200,), name='q2_token_sequence')\n",
    "emb_question2 = Embedding(1000, 64, input_length=200, name='q2_embedding')(question2_inp)\n",
    "lstm_out_qst2 = LSTM(128, name='q2_lstm')(emb_question2)\n",
    "dropout_qst2 = Dropout(0.25, name='q2_dropout')(lstm_out_qst2)\n",
    "\n",
    "concat = Concatenate(name= 'concatenation')([dropout_qst1, dropout_qst2])\n",
    "\n",
    "dense_1 = Dense(32, activation='relu', name='dense_dim_reduction')(concat)\n",
    "\n",
    "out = Dense(1, activation='sigmoid', name='dense_classification')(dense_1)\n",
    "\n",
    "model = Model(inputs=[question1_inp, question2_inp], outputs=[out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True, rankdir='TB', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_crossentropy', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    [question1_array_train, question2_array_train], \n",
    "    y_train.values, \n",
    "    epochs=10, \n",
    "    batch_size=512, \n",
    "    validation_split=.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([question1_array_test, question2_array_test], private_df.values[:, 0], batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sequence-models-demos",
   "language": "python",
   "name": "sequence-models-demos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
