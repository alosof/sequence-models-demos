{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ce notebook regroupe les étapes du préprocessing nécessaire pour passer du texte brut à un format de données exploitables par un RNN.\n",
    "- Il doit être impérativement exécuté avant le notebook `generation_model.ipynb`\n",
    "- Le corpus initial a volontairement été réduit pour ne garder que les discours prononcés par des candidats, et pour que l'on puisse vous envoyer un fichier peu volumineux :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import unidecode\n",
    "import re\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from re import IGNORECASE\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_df = pd.read_csv(\"../../datasets/speeches/train_speeches.csv\")\n",
    "speeches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombre de discours :\", speeches_df.shape[0])\n",
    "print(\"Nombre de caractères du corpus (tous discours confondus) :\", sum(speeches_df[\"discours\"].str.len()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de réduire la taille de notre vocabulaire (et donc le nombre de paramètres de notre modèle), on se propose de nettoyer le texte comme suit :\n",
    "- Passer en minuscules et en caractères non accentués\n",
    "- Remplacer toutes les ponctuations par des points\n",
    "- Remplacer tous les caractères spéciaux par des espaces\n",
    "- Remplacer les espaces multiples par un seul espace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    unaccented_text = unidecode.unidecode(text.lower())                                       # Minuscules, accents\n",
    "    wihtout_special_chars = re.sub('[\"#$%&()*+-/:<=>@[\\\\]^_`{|}~\\t\\n]', ' ', unaccented_text) # Caractères spéciaux\n",
    "    without_punct = re.sub('[!?;]', '.', wihtout_special_chars)                               # Ponctuation\n",
    "    without_extra_spaces = re.sub(' +', ' ', without_punct)                                   # Espaces en trop\n",
    "    return without_extra_spaces\n",
    "\n",
    "print(\" - Exemple avant nettoyage :\")\n",
    "print(speeches_df['discours'].iloc[0][:529])\n",
    "print()\n",
    "print(\" - Exemple après nettoyage :\")\n",
    "print(clean_text(speeches_df['discours'].iloc[0][:529]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.DataFrame(speeches_df['discours'].apply(clean_text))\n",
    "print()\n",
    "print(\"Après nettoyage de tous les discours :\")\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_size(clean_text):\n",
    "    return len(set(clean_text))\n",
    "\n",
    "print(\"Taille du vocabulaire (nombre de caractères distincts dans tout le corpus) :\", max(clean_df['discours'].apply(vocab_size).values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nous avons maintenant un texte nettoyé, avec un vocabulaire restreint. Toutefois, les modèles de machine learning traitent des données numériques et non du texte. On doit donc trouver une représentation numérique pour chaque caractère. La façon la plus simple de faire cela est le one-hot encoding.\n",
    "\n",
    "- Le one-hot encoding consiste à représenter chaque caractère comme un vecteur de taille 39 (taille de notre vocabulaire). Ce vecteur vaudra 1 à la position correspondant au caractère en question, et 0 partout ailleurs.\n",
    "\n",
    "- Dans la cellule suivante, on crée notre vocabulaire constitué des caractères distincts qu'on a gardés après nettoyage. On crée également un mapping indice unique -> caractère et le mapping inverse caratère -> indice unique. Ces mappings serviront à créer les fonctions one_hot_encode et one_hot_decode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(string.digits) + list(string.ascii_lowercase) + ['.', ' ', '\\'']\n",
    "\n",
    "index_to_char = {index: vocab[index] for index in range(len(vocab))}               # Mapping indice -> caractère\n",
    "char_to_index = {char: index for index, char in index_to_char.items()}             # Mapping caractère -> indice\n",
    "\n",
    "def one_hot_encode(text):\n",
    "    indices = [char_to_index[c] for c in text]\n",
    "    one_hot_encoding = np.zeros((len(indices), len(char_to_index)))\n",
    "    for i in range(len(indices)):\n",
    "        one_hot_encoding[i, indices[i]] = 1\n",
    "    return one_hot_encoding\n",
    "\n",
    "def one_hot_decode(encoding):\n",
    "    result = \"\"\n",
    "    for e in encoding:\n",
    "        result = result + index_to_char[list(e).index(1)]\n",
    "    return result\n",
    "\n",
    "print(\"Vocabulaire :\", vocab)\n",
    "print()\n",
    "print(\"Mapping indice -> caractère :\", index_to_char)\n",
    "print()\n",
    "print(\"Mapping caractère -> indice :\", char_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testons nos fonctions d'encodage / décodage sur un exemple de discours :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_speech = clean_df['discours'].iloc[0][:521]\n",
    "encoded_speech = one_hot_encode(sample_speech)\n",
    "decoded_code = one_hot_decode(encoded_speech)\n",
    "\n",
    "print(\" - Exemple de discours :\")\n",
    "print(sample_speech)\n",
    "print(\"-> Longueur du discours :\", len(sample_speech))\n",
    "print(\"\\n - Après one-hot encoding :\")\n",
    "print(encoded_speech)\n",
    "print(\"-> Taille de la matrice d'encodage :\", encoded_speech.shape)\n",
    "print(\"\\n - Décodage du one-hot encoding:\")\n",
    "print(decoded_code)\n",
    "print(\"-> Longueur du discours décodé :\", len(decoded_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Encodage de tous les discours... \\n\")\n",
    "\n",
    "encoded_texts = []\n",
    "for text in clean_df[\"discours\"]:\n",
    "    encoded_texts.append(one_hot_encode(text))\n",
    "\n",
    "print(encoded_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la bibliothèque Keras, un modèle de type RNN travaille sur des séquences de taille fixe. \n",
    "\n",
    "Il attend en entrée une matrice à 3 dimensions : (nombre d'échantillons, longueur des séquences, features)\n",
    "\n",
    "Dans notre cas, chaque caractère est représenté par 39 features (résultat du one-hot encoding). La dernière dimension est donc satisfaite.\n",
    "\n",
    "Toutefois, tous les discours n'ont pas la même longueur. La fonction suivante va donc permettre de découper une séquence de taille arbitraire en sous-séquences de taille fixe qui vont servir d'inputs au modèle. Par la même occasion, elle nous permettra de récupérer les targets associées à ces inputs, à savoir le caractère à prédire pour chaque input.\n",
    "\n",
    "Le dernier argument de la fonction, skip, permet de contrôler l'overlap qu'on souhaite autoriser entre les sous-séquences.\n",
    "- skip = 1 permet d'extraire toutes les sous-séquences possibles, ce qui fournit beaucoup de données mais augmente le risque d'overfitting.\n",
    "- skip = s permet de \"sauter\" s caractères à chaque extraction de sous-séquences, ce qui réduit l'overlap mais fournit moins d'inputs au modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_to_sequences(encoded_text, seq_length, skip):\n",
    "    inputs, targets = [], []\n",
    "    for i in range(0, len(encoded_text)-seq_length, skip):\n",
    "        x = list(encoded_text[i:(i+seq_length)])\n",
    "        y = encoded_text[i + seq_length]\n",
    "        inputs.append(x)\n",
    "        targets.append(y)\n",
    "    return np.array(inputs), np.array(targets)\n",
    "\n",
    "print(\"Taille de la matrice d'encodage du premier discours :\", encoded_texts[0].shape)\n",
    "cut_X, cut_y = cut_to_sequences(encoded_texts[0], 50, 5)\n",
    "print()\n",
    "print(\"Inputs après découpage en sous-séquences de taille 50 avec un skip de 3 :\", cut_X.shape)\n",
    "print(\"Targets associées :\", cut_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On applique le même traitement à tous les discours. \n",
    "\n",
    "Ensuite, on doit concaténer toutes les sous-séquences issues de ce découpage pour former une seule grande matrice de dimensions (nombre d'échantillons, longueur des séquences, features). Pour l'exercice, on ne gardera que les séquences correspondant aux 60 premiers discours car cette opération est consommatrice en mémoire et cette implémentation n'est pas la plus optimale..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_data = [cut_to_sequences(encoded_text, 50, 5) for encoded_text in encoded_texts[:60]]\n",
    "\n",
    "inputs = []\n",
    "targets = []\n",
    "\n",
    "for x, y in seq_data:\n",
    "    inputs.append(x)\n",
    "    targets.append(y)\n",
    "    \n",
    "inputs_array = np.concatenate(inputs)\n",
    "targets_array = np.concatenate(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Taille de la matrice finale d'inputs :\", inputs_array.shape)\n",
    "print(\"Taille de la matrice finale de targets :\", targets_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On sauvegarde le résultat de notre preprocessing dans des fichiers en vue de l'utiliser plus tard pour le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"../../datasets/speeches/speeches_inputs.npy\", inputs_array)\n",
    "np.save(\"../../datasets/speeches/speeches_targets.npy\", targets_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sequence-models-demos",
   "language": "python",
   "name": "sequence-models-demos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
