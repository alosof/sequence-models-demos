{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ae26f5",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5284032",
   "metadata": {},
   "source": [
    "Le **traduction automatique**, tout comme d'autres tâches de NLP,  a connu des évolutions majeures ces dernières années grâce au _Deep Learning_.\n",
    "\n",
    "Ce notebook présente un exemple simple de traduction, utilisant des **réseaux de neurones récurrents (RNN)**.\n",
    "\n",
    "Le but est d'entraîner un modèle à **traduire différents formats de dates vers un format unique**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ccf42b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a0620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from string import ascii_lowercase, digits\n",
    "from typing import List, Dict\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72e0199",
   "metadata": {},
   "source": [
    "# Données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f427a622",
   "metadata": {},
   "source": [
    "Pour cet exercice, on a généré au préalable un dataset de 25 000 dates différentes entre le `01/01/1800` et le `31/12/2050`.\n",
    "\n",
    "On a réparti ces données en deux datasets:\n",
    "- `train_data`: 10 000 pour l'entraînement du modèle\n",
    "- `test_data`: 15 000 pour l'évaluation\n",
    "\n",
    "Chaque dataset a deux colonnes:\n",
    "- `input`: la date dans l'un des différents formats sources\n",
    "- `target`: la date au format cible\n",
    "\n",
    "Commençons par charger les données et un aperçu de celles-ci:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13f43ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../../datasets/dates/train_dates.csv\", sep=\";\")\n",
    "test_data = pd.read_csv(\"../../datasets/dates/test_dates.csv\", sep=\";\")\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c5250",
   "metadata": {},
   "source": [
    "On vérifie ensuite la taille des datasets, et on s'assure qu'il n'y a aucune date en commun entre `train_data` et `test_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89e2d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Taille du dataset d'entraînement:\", len(train_data))\n",
    "print(\"Taille du dataset d'évaluation:\", len(test_data))\n",
    "print(\"Nombre de dates communes entre `train_data` et `test_data`:\", \n",
    "      len(set(train_data['target'].values).intersection(set(test_data['target'].values))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa8d91e",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dac6789",
   "metadata": {},
   "source": [
    "Les données qu'on manipule sont des **données textuelles**. \n",
    "\n",
    "Afin de pouvoir les traiter avec un modèle de Machine Learning, il faut d'abord réussir à les **transformer en _features_ (caractéristiques) numériques**. C'est l'objectif de cette section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce28450",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b5e42",
   "metadata": {},
   "source": [
    "Un texte d'entrée est d'abord **_tokenisé_**, c'est-à-dire découpé en une **séquence d'unités de texte (_tokens_)**. \n",
    "\n",
    "Pour cet exemple simple, nos unités de texte seront **les caractères**. La traduction consistera donc à prédire une séquence de caractères à partir d'une autre séquence de caractères.\n",
    "\n",
    "Il est à noter que dans les vrais modèles de traduction, les _tokens_ sont plutôt des mots, ou des parties de mots (préfixes, racines, suffixes...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa4ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(date_str: str) -> str:\n",
    "    return [c for c in date_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51adf89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_date = \"Thursday, 28 Aug. 1958\"\n",
    "tokenized_date = tokenize(example_date)\n",
    "\n",
    "print(\"Exemple de date:\")\n",
    "print(example_date)\n",
    "print()\n",
    "print(\"Résultat de la tokenisation:\")\n",
    "print(tokenized_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca43554",
   "metadata": {},
   "source": [
    "## Vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e5be18",
   "metadata": {},
   "source": [
    "L'ensemble des _tokens_ distincts dans la langue d'entrée constitue le **vocabulaire**.\n",
    "\n",
    "On a choisi de faire une **_tokenisation_ niveau caractères** afin d'avoir un vocabulaire de taille réduite. Pour le réduire encore plus, on va remplacer tous les caractères spéciaux autres que le tiret `-` par des espaces ` `, et toutes les majuscules par des minuscules. Un vocabulaire réduit simplifie la tâche de traduction, et peut donc potentiellement accélérer la convergence du modèle.\n",
    "\n",
    "- Le vocabulaire d'entrée se limite alors aux caractères alphanumériques minuscules, en plus de l'espace ` ` et du tiret `-`.\n",
    "- Le vocabulaire cible est encore plus réduit, il est constitué des chiffres de `0` à `9` et du tiret `-`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb36435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocabulary = list(digits) + list(ascii_lowercase) + [' ', '-']\n",
    "target_vocabulary = list(digits) + ['-']\n",
    "\n",
    "print(\"Vocabulaire d'entrée:\")\n",
    "print(input_vocabulary)\n",
    "print()\n",
    "print(\"Vocabulaire cible:\")\n",
    "print(target_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58cc34a",
   "metadata": {},
   "source": [
    "## Encodage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e5196",
   "metadata": {},
   "source": [
    "Un encodage possible du texte en _features_ numériques est de remplacer chaque caractère par son index dans le vocabulaire. \n",
    "\n",
    "Pour simplifier l'encodage, on va construire un mapping `(caractère > index)` pour le vocabulaire d'entrée, et de même pour le vocabulaire cible.\n",
    "\n",
    "Pour simplifier le décodage des prédictions du modèle (qui vont être numériques), on va également construire un mapping `(index > caractère)` pour le vocabulaire cible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef986f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocab_char_to_idx = {char: idx for (idx, char) in enumerate(input_vocabulary)}\n",
    "target_vocab_char_to_idx = {char: idx for (idx, char) in enumerate(target_vocabulary)}\n",
    "target_vocab_idx_to_char = {idx: char for (idx, char) in enumerate(target_vocabulary)}\n",
    "\n",
    "print(\"Mapping (caractère > index) pour le vocabulaire d'entrée:\")\n",
    "print(input_vocab_char_to_idx)\n",
    "print()\n",
    "print(\"Mapping (caractère > index) pour le vocabulaire cible:\")\n",
    "print(target_vocab_char_to_idx)\n",
    "print()\n",
    "print(\"Mapping (index > caractère) pour le vocabulaire cible:\")\n",
    "print(target_vocab_idx_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c16f188",
   "metadata": {},
   "source": [
    "Implémentons maintenant les fonctions d'encodage et de décodage pour une séquence entière. La fonction d'encodage inclut la tokenisation comme première étape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66809cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(date_str: str, char_to_idx_mapping: Dict[str, int]) -> List[int]:\n",
    "    tokenized_date = tokenize(date_str)\n",
    "    encoding = []\n",
    "    for c in tokenized_date:\n",
    "        c = c.lower()\n",
    "        c = c if c in char_to_idx_mapping else ' '\n",
    "        encoding.append(char_to_idx_mapping[c])\n",
    "    return encoding\n",
    "\n",
    "def decode(encoded_text: List[int], idx_to_char_mapping: Dict[int, str]) -> str:\n",
    "    return ''.join([idx_to_char_mapping[idx] for idx in encoded_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a05236",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_date = encode(example_date, input_vocab_char_to_idx)\n",
    "example_predicted_date = [2, 6, 10, 1, 1, 10, 2, 0, 2, 1]\n",
    "decoded_date = decode(example_predicted_date, target_vocab_idx_to_char)\n",
    "\n",
    "print(\"Exemple de date d'entrée:\")\n",
    "print(example_date)\n",
    "print()\n",
    "print(\"Résultat de l'encodage:\")\n",
    "print(encoded_date)\n",
    "print(\"----------------------------------------------------------------------------------\")\n",
    "print(\"Exemple de date prédite au format encodé:\")\n",
    "print(example_predicted_date)\n",
    "print()\n",
    "print(\"Résultat du décodage:\")\n",
    "print(decoded_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb8754c",
   "metadata": {},
   "source": [
    "L'encodage sous forme d'une séquence d'indexes donne une représentation numérique du texte, mais présente quelques inconvénients. \n",
    "\n",
    "Il instaure une \"relation d'ordre\" entre les différents tokens. Par exemple, le token `t` est associé à l'index `29`, alors que le token `d` est associé à l'index `13`.\n",
    "\n",
    "Cela peut être problématique dans certains problèmes, et rendre l'apprentissage du modèle plus difficile. On optera donc pour une représentation différente, le **one-hot encoding**, qui permet d'éliminer cette notion d'ordre entre les différentes classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7027217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(date_str: str, char_to_idx_mapping: Dict[str, int]) -> np.ndarray:\n",
    "    encoded_date = encode(date_str, char_to_idx_mapping)\n",
    "    one_hot_encoding = np.zeros((len(encoded_date), len(char_to_idx_mapping)))\n",
    "    for i, idx in enumerate(encoded_date):\n",
    "        one_hot_encoding[i, idx] = 1\n",
    "    return one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70ec89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_date = \"26-11-2021\"\n",
    "seq_encoded_date = encode(sample_date, target_vocab_char_to_idx)\n",
    "one_hot_encoded_date = one_hot(sample_date, target_vocab_char_to_idx)\n",
    "\n",
    "print(\"Exemple de date:\")\n",
    "print(sample_date)\n",
    "print()\n",
    "print(f\"Encodage au format séquence d'indexes:\")\n",
    "print(seq_encoded_date)\n",
    "print(\"Taille encodage format séquence:\", len(seq_encoded_date))\n",
    "print()\n",
    "print(f\"Encodage one-hot:\")\n",
    "print(one_hot_encoded_date)\n",
    "print(\"Taille encodage format one-hot:\", one_hot_encoded_date.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c1ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_date = \"Thursday, 1985/01/17\"\n",
    "seq_encoded_date = encode(sample_date, input_vocab_char_to_idx)\n",
    "one_hot_encoded_date = one_hot(sample_date, input_vocab_char_to_idx)\n",
    "\n",
    "print(\"Exemple de date:\")\n",
    "print(sample_date)\n",
    "print()\n",
    "print(f\"Encodage au format séquence d'indexes:\")\n",
    "print(seq_encoded_date)\n",
    "print(\"Taille encodage format séquence:\", len(seq_encoded_date))\n",
    "print()\n",
    "print(f\"Encodage one-hot:\")\n",
    "print(one_hot_encoded_date)\n",
    "print(\"Taille encodage format one-hot:\", one_hot_encoded_date.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f44522a",
   "metadata": {},
   "source": [
    "Les modèles RNN s'entraînent sur des séquences de taille fixe et prédisent des séquences de taille fixe. Or nos dates d'entrée ont des tailles différentes. \n",
    "\n",
    "On doit donc faire en sorte que toutes les séquences d'entrée aient la même taille. Pour ce faire, on va utiliser du **padding**, c'est-à-dire rajouter des zéros en début de chaque chaîne de caractère, pour obtenir une longueur uniforme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d436c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(one_hot_encoded: np.ndarray, max_len: int) -> np.ndarray:\n",
    "    sentence_len = one_hot_encoded.shape[0]\n",
    "    vocab_size = one_hot_encoded.shape[1]\n",
    "    to_add = max_len - sentence_len\n",
    "    return np.vstack([\n",
    "        np.zeros((to_add, vocab_size)),\n",
    "        one_hot_encoded\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24148d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Longueur maximale des exemples d'entraînement:\", train_data['input'].str.len().max())\n",
    "print(\"Longueur maximale des exemples d'évaluation:\", test_data['input'].str.len().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79286f4",
   "metadata": {},
   "source": [
    "On va donc appliquer un padding à toutes nos séquences d'entrée, pour qu'elles aient toute une longueur de 28.\n",
    "\n",
    "Ecrivons une fonction de preprocessing globale qui enchaîne **one-hot encoding** et **zéro-padding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc1187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(date_str: str, char_to_idx_mapping: Dict[str, int], max_len: int):\n",
    "    return pad(one_hot(date_str, char_to_idx_mapping), max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e978f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_date = \"Thursday, 1985/01/17\"\n",
    "preprocessed_date = preprocess(sample_date, input_vocab_char_to_idx, 28)\n",
    "\n",
    "print(\"Exemple de date:\")\n",
    "print(sample_date)\n",
    "print()\n",
    "print(f\"Encodage one-hot et padding:\")\n",
    "print(preprocessed_date)\n",
    "print(\"Taille de la représentation numérique:\", preprocessed_date.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1587f109",
   "metadata": {},
   "source": [
    "Il n'y a plus qu'à appliquer cette même transformation à toutes les dates de notre dataset pour uniformiser les entrées et les sorties du modèle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87246b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.vstack(train_data['input'].map(\n",
    "    lambda d: np.expand_dims(preprocess(d, input_vocab_char_to_idx, 28), 0)\n",
    "))\n",
    "y_train = np.vstack(train_data['target'].map(\n",
    "    lambda d: np.expand_dims(preprocess(d, target_vocab_char_to_idx, 10), 0)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce49d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.vstack(test_data['input'].map(\n",
    "    lambda d: np.expand_dims(preprocess(d, input_vocab_char_to_idx, 28), 0)\n",
    "))\n",
    "y_test = np.vstack(test_data['target'].map(\n",
    "    lambda d: np.expand_dims(preprocess(d, target_vocab_char_to_idx, 10), 0)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRAIN - Données d'entrée:\", X_train.shape)\n",
    "print(\"TRAIN - Données de sortie:\", y_train.shape)\n",
    "print(\"TEST - Données d'entrée:\", X_test.shape)\n",
    "print(\"TEST - Données de sortie:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8a4427",
   "metadata": {},
   "source": [
    "# Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284fcf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(28, 38)))\n",
    "model.add(RepeatVector(10))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(11, activation='softmax')))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad688e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, show_shapes=True, dpi=150, rankdir='TB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49052598",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary(line_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01643148",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1552d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7535738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_DATE = \"Ven. 26 nov. 2021\"\n",
    "\n",
    "prediction = model.predict(\n",
    "    np.expand_dims(pad(to_numeric_representation(TEST_DATE, input_vocabulary), 28), 0)\n",
    ")\n",
    "\n",
    "print(''.join([reverse_target_vocabulary[idx] for idx in prediction.argmax(axis=-1)[0]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sequence-models-demos",
   "language": "python",
   "name": "sequence-models-demos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
