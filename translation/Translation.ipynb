{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29ae26f5",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5284032",
   "metadata": {},
   "source": [
    "Le **traduction automatique**, tout comme d'autres tâches de NLP,  a connu des évolutions majeures ces dernières années grâce au _Deep Learning_.\n",
    "\n",
    "Ce notebook présente un exemple simple de traduction, utilisant des **réseaux de neurones récurrents (RNN)**.\n",
    "\n",
    "Le but est d'entraîner un modèle à **traduire différents formats de dates vers un format unique**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ccf42b",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c9a0620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from string import ascii_lowercase, digits\n",
    "from typing import List, Dict\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72e0199",
   "metadata": {},
   "source": [
    "# Données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f427a622",
   "metadata": {},
   "source": [
    "Pour cet exercice, on a généré au préalable un dataset de 25 000 dates différentes entre le `01/01/1800` et le `31/12/2050`.\n",
    "\n",
    "On a réparti ces données en deux datasets:\n",
    "- `train_data`: 10 000 pour l'entraînement du modèle\n",
    "- `test_data`: 15 000 pour l'évaluation\n",
    "\n",
    "Chaque dataset a deux colonnes:\n",
    "- `input`: la date dans l'un des différents formats sources\n",
    "- `target`: la date au format cible\n",
    "\n",
    "Commençons par charger les données et un aperçu de celles-ci:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d13f43ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sunday 17 Jun 1866</td>\n",
       "      <td>17-06-1866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thursday, 28 Aug. 1958</td>\n",
       "      <td>28-08-1958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thursday, 1985/01/17</td>\n",
       "      <td>17-01-1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1937 03 24</td>\n",
       "      <td>24-03-1937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Saturday, 1831 April 09</td>\n",
       "      <td>09-04-1831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     input      target\n",
       "0       Sunday 17 Jun 1866  17-06-1866\n",
       "1   Thursday, 28 Aug. 1958  28-08-1958\n",
       "2     Thursday, 1985/01/17  17-01-1985\n",
       "3               1937 03 24  24-03-1937\n",
       "4  Saturday, 1831 April 09  09-04-1831"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv(\"./data/train_dates.csv\", sep=\";\")\n",
    "test_data = pd.read_csv(\"./data/test_dates.csv\", sep=\";\")\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c5250",
   "metadata": {},
   "source": [
    "On vérifie ensuite la taille des datasets, et on s'assure qu'il n'y a aucune date en commun entre `train_data` et `test_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c89e2d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dataset d'entraînement: 10000\n",
      "Taille du dataset d'évaluation: 15000\n",
      "Nombre de dates communes entre `train_data` et `test_data`: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Taille du dataset d'entraînement:\", len(train_data))\n",
    "print(\"Taille du dataset d'évaluation:\", len(test_data))\n",
    "print(\"Nombre de dates communes entre `train_data` et `test_data`:\", \n",
    "      len(set(train_data['target'].values).intersection(set(test_data['target'].values))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa8d91e",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dac6789",
   "metadata": {},
   "source": [
    "Les données qu'on manipule sont des **données textuelles**. \n",
    "\n",
    "Afin de pouvoir les traiter avec un modèle de Machine Learning, il faut d'abord réussir à les **transformer en _features_ (caractéristiques) numériques**. C'est l'objectif de cette section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce28450",
   "metadata": {},
   "source": [
    "## Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b5e42",
   "metadata": {},
   "source": [
    "Un texte d'entrée est d'abord **_tokenisé_**, c'est-à-dire découpé en une **séquence d'unités de texte (_tokens_)**. \n",
    "\n",
    "Pour cet exemple simple, nos unités de texte seront **les caractères**. La traduction consistera donc à prédire une séquence de caractères à partir d'une autre séquence de caractères.\n",
    "\n",
    "Il est à noter que dans les vrais modèles de traduction, les _tokens_ sont plutôt des mots, ou des parties de mots (préfixes, racines, suffixes...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "daa4ac73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(date_str: str) -> str:\n",
    "    return [c for c in date_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "51adf89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de date:\n",
      "Thursday, 28 Aug. 1958\n",
      "\n",
      "Résultat de la tokenisation:\n",
      "['T', 'h', 'u', 'r', 's', 'd', 'a', 'y', ',', ' ', '2', '8', ' ', 'A', 'u', 'g', '.', ' ', '1', '9', '5', '8']\n"
     ]
    }
   ],
   "source": [
    "example_date = \"Thursday, 28 Aug. 1958\"\n",
    "tokenized_date = tokenize(example_date)\n",
    "\n",
    "print(\"Exemple de date:\")\n",
    "print(example_date)\n",
    "print()\n",
    "print(\"Résultat de la tokenisation:\")\n",
    "print(tokenized_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca43554",
   "metadata": {},
   "source": [
    "## Vocabulaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e5be18",
   "metadata": {},
   "source": [
    "L'ensemble des _tokens_ distincts dans la langue d'entrée constitue le **vocabulaire**.\n",
    "\n",
    "On a choisi de faire une **_tokenisation_ niveau caractères** afin d'avoir un vocabulaire de taille réduite. Pour le réduire encore plus, on va remplacer tous les caractères spéciaux autres que le tiret `-` par des espaces ` `, et toutes les majuscules par des minuscules. Un vocabulaire réduit simplifie la tâche de traduction, et peut donc potentiellement accélérer la convergence du modèle.\n",
    "\n",
    "- Le vocabulaire d'entrée se limite alors aux caractères alphanumériques minuscules, en plus de l'espace ` ` et du tiret `-`.\n",
    "- Le vocabulaire cible est encore plus réduit, il est constitué des chiffres de `0` à `9` et du tiret `-`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "cb36435d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulaire d'entrée:\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ', '-']\n",
      "\n",
      "Vocabulaire cible:\n",
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '-']\n"
     ]
    }
   ],
   "source": [
    "input_vocabulary = list(digits) + list(ascii_lowercase) + [' ', '-']\n",
    "target_vocabulary = list(digits) + ['-']\n",
    "\n",
    "print(\"Vocabulaire d'entrée:\")\n",
    "print(input_vocabulary)\n",
    "print()\n",
    "print(\"Vocabulaire cible:\")\n",
    "print(target_vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58cc34a",
   "metadata": {},
   "source": [
    "## Encodage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e5196",
   "metadata": {},
   "source": [
    "Un encodage possible du texte en _features_ numériques est de remplacer chaque caractère par son index dans le vocabulaire. \n",
    "\n",
    "Pour simplifier l'encodage, on va construire un mapping `(caractère > index)` pour le vocabulaire d'entrée, et de même pour le vocabulaire cible.\n",
    "\n",
    "Pour simplifier le décodage des prédictions du modèle (qui vont être numériques), on va également construire un mapping `(index > caractère)` pour le vocabulaire cible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8ef986f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping (caractère > index) pour le vocabulaire d'entrée:\n",
      "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, 'a': 10, 'b': 11, 'c': 12, 'd': 13, 'e': 14, 'f': 15, 'g': 16, 'h': 17, 'i': 18, 'j': 19, 'k': 20, 'l': 21, 'm': 22, 'n': 23, 'o': 24, 'p': 25, 'q': 26, 'r': 27, 's': 28, 't': 29, 'u': 30, 'v': 31, 'w': 32, 'x': 33, 'y': 34, 'z': 35, ' ': 36, '-': 37}\n",
      "\n",
      "Mapping (caractère > index) pour le vocabulaire cible:\n",
      "{'0': 0, '1': 1, '2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, '-': 10}\n",
      "\n",
      "Mapping (index > caractère) pour le vocabulaire cible:\n",
      "{0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9', 10: '-'}\n"
     ]
    }
   ],
   "source": [
    "input_vocab_char_to_idx = {char: idx for (idx, char) in enumerate(input_vocabulary)}\n",
    "target_vocab_char_to_idx = {char: idx for (idx, char) in enumerate(target_vocabulary)}\n",
    "target_vocab_idx_to_char = {idx: char for (idx, char) in enumerate(target_vocabulary)}\n",
    "\n",
    "print(\"Mapping (caractère > index) pour le vocabulaire d'entrée:\")\n",
    "print(input_vocab_char_to_idx)\n",
    "print()\n",
    "print(\"Mapping (caractère > index) pour le vocabulaire cible:\")\n",
    "print(target_vocab_char_to_idx)\n",
    "print()\n",
    "print(\"Mapping (index > caractère) pour le vocabulaire cible:\")\n",
    "print(target_vocab_idx_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c16f188",
   "metadata": {},
   "source": [
    "Implémentons maintenant les fonctions d'encodage et de décodage pour une séquence entière. La fonction d'encodage inclut la tokenisation comme première étape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "66809cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(date_str: str, char_to_idx_mapping: Dict[str, int]) -> List[int]:\n",
    "    tokenized_date = tokenize(date_str)\n",
    "    encoding = []\n",
    "    for c in tokenized_date:\n",
    "        c = c.lower()\n",
    "        c = c if c in char_to_idx_mapping else ' '\n",
    "        encoding.append(char_to_idx_mapping[c])\n",
    "    return encoding\n",
    "\n",
    "def decode(encoded_text: List[int], idx_to_char_mapping: Dict[int, str]) -> str:\n",
    "    return ''.join([idx_to_char_mapping[idx] for idx in encoded_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "84a05236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple de date d'entrée:\n",
      "Thursday, 28 Aug. 1958\n",
      "\n",
      "Résultat de l'encodage:\n",
      "[29, 17, 30, 27, 28, 13, 10, 34, 36, 36, 2, 8, 36, 10, 30, 16, 36, 36, 1, 9, 5, 8]\n",
      "\n",
      "Exemple de date prédite au format encodé:\n",
      "[2, 6, 10, 1, 1, 10, 2, 0, 2, 1]\n",
      "\n",
      "Résultat du décodage:\n",
      "26-11-2021\n"
     ]
    }
   ],
   "source": [
    "encoded_date = encode(tokenized_date, input_vocab_char_to_idx)\n",
    "example_predicted_date = [2, 6, 10, 1, 1, 10, 2, 0, 2, 1]\n",
    "decoded_date = decode(example_predicted_date, target_vocab_idx_to_char)\n",
    "\n",
    "print(\"Exemple de date d'entrée:\")\n",
    "print(example_date)\n",
    "print()\n",
    "print(\"Résultat de l'encodage:\")\n",
    "print(encoded_date)\n",
    "print()\n",
    "print(\"Exemple de date prédite au format encodé:\")\n",
    "print(example_predicted_date)\n",
    "print()\n",
    "print(\"Résultat du décodage:\")\n",
    "print(decoded_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7027217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(encoded_date: List[int], vocab_size: int) -> np.ndarray:\n",
    "    one_hot_encoding = np.zeros((len(encoded_date), vocab_size))\n",
    "    for i, idx in enumerate(encoded_date):\n",
    "        one_hot_encoding[i, idx] = 1\n",
    "    return one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d70ec89c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot([2, 6, 10, 1, 1, 10, 2, 0, 2, 1], 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e5ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0dea56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756bd311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d75524b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa8e73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cb7463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc8229e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numeric_representation(date_str: str, vocabulary: dict, verbose: bool = False) -> np.ndarray:\n",
    "    date_lower = date_str.lower()\n",
    "    tokenized_date = [c for c in date_lower]\n",
    "    tokenized_date_withtout_special_chars = [c if c in vocabulary else ' ' for c in tokenized_date]\n",
    "    numeric_tokens = [vocabulary[c] for c in tokenized_date_withtout_special_chars]\n",
    "    one_hot_encoding = np.array([one_hot(c, len(vocabulary)) for c in numeric_tokens])\n",
    "    if verbose:\n",
    "        print(\"Texte initial:\\n\", date_str, '\\n')\n",
    "        print(\"Texte sans majuscules:\\n\", date_lower, '\\n')\n",
    "        print(\"Tokens (caractères):\\n\", tokenized_date, '\\n')\n",
    "        print(\"Tokens sans caractères spéciaux:\\n\", tokenized_date_withtout_special_chars, '\\n')\n",
    "        print(\"Tokens remplacés par leurs indexes:\\n\", numeric_tokens, '\\n')\n",
    "        print(\"One-hot encoding:\\n\", one_hot_encoding)\n",
    "    return one_hot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9c1ffea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte initial:\n",
      " Thursday, 1985/01/17 \n",
      "\n",
      "Texte sans majuscules:\n",
      " thursday, 1985/01/17 \n",
      "\n",
      "Tokens (caractères):\n",
      " ['t', 'h', 'u', 'r', 's', 'd', 'a', 'y', ',', ' ', '1', '9', '8', '5', '/', '0', '1', '/', '1', '7'] \n",
      "\n",
      "Tokens sans caractères spéciaux:\n",
      " ['t', 'h', 'u', 'r', 's', 'd', 'a', 'y', ' ', ' ', '1', '9', '8', '5', ' ', '0', '1', ' ', '1', '7'] \n",
      "\n",
      "Tokens remplacés par leurs indexes:\n",
      " [29, 17, 30, 27, 28, 13, 10, 34, 36, 36, 1, 9, 8, 5, 36, 0, 1, 36, 1, 7] \n",
      "\n",
      "One-hot encoding:\n",
      " [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "example = to_numeric_representation(date_str='Thursday, 1985/01/17', vocabulary=input_vocabulary, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8cdea3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 38)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d436c8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(one_hot_encoded: np.ndarray, max_len: int) -> np.ndarray:\n",
    "    sentence_len = one_hot_encoded.shape[0]\n",
    "    vocab_size = one_hot_encoded.shape[1]\n",
    "    to_add = max_len - sentence_len\n",
    "    return np.vstack([\n",
    "        np.zeros((to_add, vocab_size)),\n",
    "        one_hot_encoded\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "24148d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longueur maximale des exemples d'entraînement: 28\n",
      "Longueur maximale des exemples d'évaluation: 28\n"
     ]
    }
   ],
   "source": [
    "print(\"Longueur maximale des exemples d'entraînement:\", train_data['input'].str.len().max())\n",
    "print(\"Longueur maximale des exemples d'évaluation:\", test_data['input'].str.len().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e726bf92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 38)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_example = pad(example, 28)\n",
    "padded_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffc74d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_numeric_representation(\"17-06-1866\", target_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "87246b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.vstack(train_data['input'].map(\n",
    "    lambda d: np.expand_dims(pad(to_numeric_representation(d, input_vocabulary), 28), 0)\n",
    "))\n",
    "y_train = np.vstack(train_data['target'].map(\n",
    "    lambda d: np.expand_dims(pad(to_numeric_representation(d, target_vocabulary), 10), 0)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7ce49d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.vstack(test_data['input'].map(\n",
    "    lambda d: np.expand_dims(pad(to_numeric_representation(d, input_vocabulary), 28), 0)\n",
    "))\n",
    "y_test = np.vstack(test_data['target'].map(\n",
    "    lambda d: np.expand_dims(pad(to_numeric_representation(d, target_vocabulary), 10), 0)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b469364e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 38)\n",
      "(10000, 10, 11)\n",
      "(15000, 28, 38)\n",
      "(15000, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8a4427",
   "metadata": {},
   "source": [
    "# Modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "284fcf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(28, 38)))\n",
    "model.add(RepeatVector(10))\n",
    "model.add(LSTM(32, return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(11, activation='softmax')))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "49052598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "____________________________________________________________________________________________________\n",
      " Layer (type)                                Output Shape                            Param #        \n",
      "====================================================================================================\n",
      " lstm_4 (LSTM)                               (None, 64)                              26368          \n",
      "                                                                                                    \n",
      " repeat_vector_2 (RepeatVector)              (None, 10, 64)                          0              \n",
      "                                                                                                    \n",
      " lstm_5 (LSTM)                               (None, 10, 32)                          12416          \n",
      "                                                                                                    \n",
      " time_distributed_2 (TimeDistributed)        (None, 10, 11)                          363            \n",
      "                                                                                                    \n",
      "====================================================================================================\n",
      "Total params: 39,147\n",
      "Trainable params: 39,147\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary(line_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "01643148",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "188/188 [==============================] - 5s 15ms/step - loss: 1.9976 - accuracy: 0.3031 - val_loss: 1.7676 - val_accuracy: 0.3929\n",
      "Epoch 2/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 1.6354 - accuracy: 0.4554 - val_loss: 1.4699 - val_accuracy: 0.5629\n",
      "Epoch 3/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 1.2856 - accuracy: 0.5996 - val_loss: 1.1391 - val_accuracy: 0.6239\n",
      "Epoch 4/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 1.0316 - accuracy: 0.6429 - val_loss: 0.9454 - val_accuracy: 0.6611\n",
      "Epoch 5/50\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 0.9025 - accuracy: 0.6713 - val_loss: 0.8803 - val_accuracy: 0.6735\n",
      "Epoch 6/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.8286 - accuracy: 0.6887 - val_loss: 0.8039 - val_accuracy: 0.6922\n",
      "Epoch 7/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.7777 - accuracy: 0.7026 - val_loss: 0.7596 - val_accuracy: 0.7073\n",
      "Epoch 8/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.7350 - accuracy: 0.7141 - val_loss: 0.7182 - val_accuracy: 0.7196\n",
      "Epoch 9/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.6938 - accuracy: 0.7277 - val_loss: 0.6822 - val_accuracy: 0.7316\n",
      "Epoch 10/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.6510 - accuracy: 0.7442 - val_loss: 0.6394 - val_accuracy: 0.7492\n",
      "Epoch 11/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.6139 - accuracy: 0.7584 - val_loss: 0.6163 - val_accuracy: 0.7584\n",
      "Epoch 12/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.5797 - accuracy: 0.7740 - val_loss: 0.5707 - val_accuracy: 0.7775\n",
      "Epoch 13/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.5462 - accuracy: 0.7887 - val_loss: 0.5317 - val_accuracy: 0.7933\n",
      "Epoch 14/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.5123 - accuracy: 0.8022 - val_loss: 0.5105 - val_accuracy: 0.8045\n",
      "Epoch 15/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.4744 - accuracy: 0.8211 - val_loss: 0.4651 - val_accuracy: 0.8228\n",
      "Epoch 16/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.4393 - accuracy: 0.8347 - val_loss: 0.4439 - val_accuracy: 0.8335\n",
      "Epoch 17/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.4140 - accuracy: 0.8462 - val_loss: 0.4116 - val_accuracy: 0.8468\n",
      "Epoch 18/50\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 0.3765 - accuracy: 0.8630 - val_loss: 0.3807 - val_accuracy: 0.8592\n",
      "Epoch 19/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.3488 - accuracy: 0.8740 - val_loss: 0.3465 - val_accuracy: 0.8742\n",
      "Epoch 20/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.3134 - accuracy: 0.8903 - val_loss: 0.3237 - val_accuracy: 0.8849\n",
      "Epoch 21/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.2864 - accuracy: 0.9004 - val_loss: 0.2968 - val_accuracy: 0.8943\n",
      "Epoch 22/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.2552 - accuracy: 0.9139 - val_loss: 0.2671 - val_accuracy: 0.9072\n",
      "Epoch 23/50\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 0.2338 - accuracy: 0.9233 - val_loss: 0.2396 - val_accuracy: 0.9194\n",
      "Epoch 24/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.2081 - accuracy: 0.9326 - val_loss: 0.2103 - val_accuracy: 0.9307\n",
      "Epoch 25/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.1872 - accuracy: 0.9409 - val_loss: 0.1956 - val_accuracy: 0.9352\n",
      "Epoch 26/50\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 0.1669 - accuracy: 0.9486 - val_loss: 0.1746 - val_accuracy: 0.9433\n",
      "Epoch 27/50\n",
      "188/188 [==============================] - 2s 11ms/step - loss: 0.1473 - accuracy: 0.9565 - val_loss: 0.1634 - val_accuracy: 0.9461\n",
      "Epoch 28/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.1337 - accuracy: 0.9618 - val_loss: 0.1442 - val_accuracy: 0.9546\n",
      "Epoch 29/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.1182 - accuracy: 0.9673 - val_loss: 0.1321 - val_accuracy: 0.9597\n",
      "Epoch 30/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.1060 - accuracy: 0.9721 - val_loss: 0.1187 - val_accuracy: 0.9638\n",
      "Epoch 31/50\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 0.0931 - accuracy: 0.9761 - val_loss: 0.1068 - val_accuracy: 0.9692\n",
      "Epoch 32/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.0822 - accuracy: 0.9805 - val_loss: 0.0963 - val_accuracy: 0.9722\n",
      "Epoch 33/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.0727 - accuracy: 0.9836 - val_loss: 0.0859 - val_accuracy: 0.9753\n",
      "Epoch 34/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.0649 - accuracy: 0.9861 - val_loss: 0.0797 - val_accuracy: 0.9781\n",
      "Epoch 35/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.0567 - accuracy: 0.9882 - val_loss: 0.0783 - val_accuracy: 0.9773\n",
      "Epoch 36/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.0506 - accuracy: 0.9902 - val_loss: 0.0710 - val_accuracy: 0.9805\n",
      "Epoch 37/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.0451 - accuracy: 0.9917 - val_loss: 0.0544 - val_accuracy: 0.9859\n",
      "Epoch 38/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.0378 - accuracy: 0.9947 - val_loss: 0.0488 - val_accuracy: 0.9886\n",
      "Epoch 39/50\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 0.0326 - accuracy: 0.9958 - val_loss: 0.0448 - val_accuracy: 0.9895\n",
      "Epoch 40/50\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 0.0295 - accuracy: 0.9964 - val_loss: 0.0453 - val_accuracy: 0.9883\n",
      "Epoch 41/50\n",
      "188/188 [==============================] - 2s 13ms/step - loss: 0.0260 - accuracy: 0.9972 - val_loss: 0.0366 - val_accuracy: 0.9922\n",
      "Epoch 42/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.0221 - accuracy: 0.9982 - val_loss: 0.0346 - val_accuracy: 0.9923\n",
      "Epoch 43/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.0255 - accuracy: 0.9964 - val_loss: 0.0555 - val_accuracy: 0.9835\n",
      "Epoch 44/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.0216 - accuracy: 0.9976 - val_loss: 0.0261 - val_accuracy: 0.9948\n",
      "Epoch 45/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.0149 - accuracy: 0.9991 - val_loss: 0.0253 - val_accuracy: 0.9954\n",
      "Epoch 46/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.0136 - accuracy: 0.9992 - val_loss: 0.0219 - val_accuracy: 0.9959\n",
      "Epoch 47/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.0110 - accuracy: 0.9997 - val_loss: 0.0224 - val_accuracy: 0.9949\n",
      "Epoch 48/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.0174 - accuracy: 0.9976 - val_loss: 0.0202 - val_accuracy: 0.9960\n",
      "Epoch 49/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.0101 - accuracy: 0.9995 - val_loss: 0.0217 - val_accuracy: 0.9958\n",
      "Epoch 50/50\n",
      "188/188 [==============================] - 2s 12ms/step - loss: 0.0082 - accuracy: 0.9999 - val_loss: 0.0148 - val_accuracy: 0.9973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd55f0ad250>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1552d682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 2s 3ms/step - loss: 0.0154 - accuracy: 0.9973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.015441731549799442, 0.9972599744796753]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7535738e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26-11-2021\n"
     ]
    }
   ],
   "source": [
    "TEST_DATE = \"Ven. 26 nov. 2021\"\n",
    "\n",
    "prediction = model.predict(\n",
    "    np.expand_dims(pad(to_numeric_representation(TEST_DATE, input_vocabulary), 28), 0)\n",
    ")\n",
    "\n",
    "print(''.join([reverse_target_vocabulary[idx] for idx in prediction.argmax(axis=-1)[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7978ce66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sequence-models-demos",
   "language": "python",
   "name": "sequence-models-demos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
